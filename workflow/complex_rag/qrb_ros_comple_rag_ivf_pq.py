
# -*- coding: utf-8 -*-
# æœ¬æ–‡ä»¶ç”¨äºæ•™å­¦ç›®çš„ï¼Œ**ä¿®å¤ IVF-PQ åœ¨å¤§è§„æ¨¡æ–‡æœ¬ä¸‹å¡æ­»çš„é—®é¢˜**
# This file fixes the IVF-PQ â€œhangâ€ issue on large text input
# ä»…ä½¿ç”¨ numpyï¼Œå®Œæ•´è®²è§£ IVF-PQ çš„å·¥ç¨‹ç»†èŠ‚
# Only numpy is used, complete engineering-level IVF-PQ explanation

import numpy as np  # æ•°å€¼è®¡ç®—åº“ / Numerical computation library
import os  # æ–‡ä»¶ç³»ç»Ÿæ“ä½œ / File system operations


def chunk_utf8_file(path, chunk_bytes=512):
    # æŒ‰ UTF-8 å­—èŠ‚åˆ‡åˆ†è¶…å¤§ä¸­æ–‡æ–‡æœ¬
    # Chunk huge UTF-8 Chinese text file by byte length
    print(f"ğŸ“‚ [chunk_utf8_file] path={path}, chunk_bytes={chunk_bytes}")

    chunks = []  # æ–‡æœ¬å—åˆ—è¡¨ / Text chunks list
    with open(path, "rb") as f:  # äºŒè¿›åˆ¶è¯»å– / Binary read
        buffer = b""  # UTF-8 ç¼“å†²åŒº / UTF-8 buffer
        while True:
            data = f.read(chunk_bytes)
            if not data:
                break
            buffer += data
            try:
                txt = buffer.decode("utf-8")
                chunks.append(txt)
                buffer = b""
            except UnicodeDecodeError:
                pass

    if buffer:
        chunks.append(buffer.decode("utf-8"))

    print(f"âœ… [chunk_utf8_file] total_chunks={len(chunks)}")
    return chunks


def text_to_vector(text, dim):
    # å°†æ–‡æœ¬æ˜ å°„ä¸ºå›ºå®šç»´åº¦å‘é‡ï¼ˆhash embeddingï¼‰
    # Map text to fixed-dimension vector using hash embedding
    vec = np.zeros(dim, dtype=np.float32)
    for ch in text:
        vec[ord(ch) % dim] += 1.0
    norm = np.linalg.norm(vec)
    if norm > 0:
        vec /= norm
    return vec


def l2_distance(a, b):
    # è®¡ç®— L2 è·ç¦»ï¼ˆå‘é‡åŒ–ç‰ˆæœ¬ï¼‰
    # Compute L2 distance (vectorized)
    return np.linalg.norm(a - b)


def kmeans(data, k, iters=25, verbose=True):
    # âœ… ä¿®å¤ç‚¹ï¼šKMeans å®Œå…¨å‘é‡åŒ–ï¼Œé¿å… Python for åµŒå¥—å¯¼è‡´å¡æ­»
    # âœ… Fix: Fully vectorized KMeans to avoid Python for-loop freeze
    print(f"ğŸš€ [kmeans] k={k}, iters={iters}, samples={len(data)}")

    n, d = data.shape
    centroids = data[np.random.choice(n, k, replace=False)]

    for it in range(iters):
        # ä½¿ç”¨å¹¿æ’­è®¡ç®—è·ç¦»çŸ©é˜µ (n, k)
        # Compute distance matrix via broadcasting
        dists = np.linalg.norm(data[:, None, :] - centroids[None, :, :], axis=2)
        labels = np.argmin(dists, axis=1)

        for i in range(k):
            mask = labels == i
            if np.any(mask):
                centroids[i] = data[mask].mean(axis=0)

        if verbose and it % 5 == 0:
            print(f"â±ï¸ [kmeans] iter={it}")

    print("âœ… [kmeans] finished")
    return centroids



def train_pq(data, m, ks):
    # è®­ç»ƒ Product Quantizationï¼ˆPQï¼‰å­ç©ºé—´ç æœ¬
    # Train Product Quantization (PQ) subspace codebooks
    #
    # PQ çš„æ ¸å¿ƒæ€æƒ³ï¼š
    # 1ï¼‰å°†åŸå§‹ d ç»´å‘é‡æŒ‰ç»´åº¦åˆ‡åˆ†ä¸º m ä¸ªäº’ä¸é‡å çš„å­ç©ºé—´
    # 2ï¼‰åœ¨æ¯ä¸ªå­ç©ºé—´ä¸Šåˆ†åˆ«è®­ç»ƒä¸€ä¸ª KMeans é‡åŒ–å™¨
    # 3ï¼‰æ¯ä¸ªå­ç©ºé—´åªä¿å­˜ä¸€ä¸ªâ€œä¸­å¿ƒç´¢å¼•â€ï¼Œæ˜¾è‘—å‹ç¼©å­˜å‚¨
    #
    # Core idea of PQ:
    # 1) Split original d-dim vectors into m disjoint subspaces
    # 2) Train an independent KMeans quantizer on each subspace
    # 3) Store only centroid indices for compression

    print(f"ğŸ§© [train_pq] m={m}, ks={ks}, samples={len(data)}")  # æ‰“å°å…³é”®å‚æ•° / Print key parameters

    n, d = data.shape  # n ä¸ºå‘é‡æ•°é‡ï¼Œd ä¸ºå‘é‡ç»´åº¦ / n = number of vectors, d = dimension
    assert d % m == 0  # æ¯ä¸ªå­ç©ºé—´å¿…é¡»ç­‰åˆ† / Each subspace must divide dimension evenly

    subdim = d // m  # æ¯ä¸ªå­ç©ºé—´çš„ç»´åº¦ / Dimension of each subspace
    # ç¤ºä¾‹ï¼šd=128, m=8 â†’ subdim=16
    # Example: d=128, m=8 â†’ subdim=16

    codebooks = []  # ç”¨äºä¿å­˜æ‰€æœ‰å­ç©ºé—´çš„ç æœ¬ / Store all subspace codebooks

    for i in range(m):
        # éå†ç¬¬ i ä¸ªå­ç©ºé—´
        # Iterate over the i-th subspace

        print(f"ğŸ”¹ [train_pq] subspace={i}, dim={subdim}")  # å­ç©ºé—´çº§åˆ«æ‰“å° / Subspace-level print

        # ä»æ‰€æœ‰å‘é‡ä¸­åˆ‡å‡ºå½“å‰å­ç©ºé—´çš„åˆ†é‡
        # Slice the i-th subspace component from all vectors
        #
        # å½¢çŠ¶å˜åŒ–ï¼š
        # data:     (n, d)
        # sub_data: (n, subdim)
        sub_data = data[:, i * subdim:(i + 1) * subdim]

        # åœ¨å½“å‰å­ç©ºé—´ä¸Šè®­ç»ƒ KMeans
        # Train KMeans on the current subspace
        #
        # ks = å­ç©ºé—´ä¸­ä½¿ç”¨çš„èšç±»ä¸­å¿ƒæ•°é‡
        # ks = number of centroids in this subspace
        #
        # æ¯ä¸€ä¸ªä¸­å¿ƒä»£è¡¨ä¸€ä¸ªâ€œå±€éƒ¨åŸå‹å‘é‡â€
        # Each centroid represents a local prototype vector
        centers = kmeans(sub_data, ks, iters=20)

        # å°†è¯¥å­ç©ºé—´çš„ç æœ¬åŠ å…¥åˆ—è¡¨
        # Append this subspace codebook
        #
        # æœ€ç»ˆ codebooks çš„ç»“æ„ï¼š
        # codebooks[m][ks][subdim]
        # codebooks shape: (m, ks, subdim)
        codebooks.append(centers)

    # è‡³æ­¤ï¼ŒPQ è®­ç»ƒå®Œæˆï¼š
    # æ¯ä¸ªå­ç©ºé—´éƒ½æœ‰ä¸€ä¸ªç‹¬ç«‹çš„ KMeans ç æœ¬
    # At this point, PQ training is finished:
    # Each subspace has its own independent KMeans codebook

    print("âœ… [train_pq] all subspace codebooks trained")
    return codebooks  # è¿”å› PQ ç æœ¬ / Return PQ codebooks




def pq_encode_batch(vectors, codebooks):
    # å¯¹ä¸€æ‰¹å‘é‡è¿›è¡Œ Product Quantizationï¼ˆPQï¼‰ç¼–ç 
    # Encode a batch of vectors using Product Quantization (PQ)
    #
    # è¯¥å‡½æ•°æ˜¯ PQ åœ¨å·¥ç¨‹å®ç°ä¸­çš„å…³é”®æ€§èƒ½èŠ‚ç‚¹ï¼š
    # This function is a critical performance hotspot in PQ engineering.
    #
    # ç›®æ ‡ / Goalï¼š
    # å°†æ¯ä¸ªé«˜ç»´å‘é‡ x âˆˆ R^d è½¬æ¢ä¸º m ä¸ªæ•´æ•°ç´¢å¼•ï¼š
    # Convert each high-dimensional vector x âˆˆ R^d into m integer indices:
    #
    #     x â†’ [câ‚€, câ‚, ..., c_{m-1}]
    #
    # å…¶ä¸­ c_i è¡¨ç¤ºï¼š
    # where c_i denotes:
    # â€œx åœ¨ç¬¬ i ä¸ªå­ç©ºé—´ä¸­ï¼Œæœ€æ¥è¿‘å“ªä¸ªå­ç©ºé—´ä¸­å¿ƒâ€
    # "which subspace centroid x is closest to in the i-th subspace"

    print(f"ğŸ§± [pq_encode_batch] vectors={len(vectors)}, subspaces={len(codebooks)}")
    # æ‰“å°æ‰¹é‡å¤§å°ä¸å­ç©ºé—´æ•°é‡ / Print batch size and number of subspaces

    n, d = vectors.shape
    # nï¼šå‘é‡æ•°é‡ / number of vectors
    # dï¼šåŸå§‹å‘é‡æ€»ç»´åº¦ / original vector dimension

    m = len(codebooks)
    # mï¼šPQ å­ç©ºé—´æ•°é‡ / number of PQ subspaces

    assert d % m == 0
    # æ–­è¨€ç»´åº¦å¯æ•´é™¤ / Ensure dimension is divisible by subspaces

    subdim = d // m
    # subdimï¼šæ¯ä¸ªå­ç©ºé—´çš„ç»´åº¦ / dimension per subspace

    codes = np.zeros((n, m), dtype=np.int32)
    # åˆå§‹åŒ– PQ ç¼–ç çŸ©é˜µ
    # Initialize PQ code matrix
    #
    # codes[i, j] = ç¬¬ i ä¸ªå‘é‡åœ¨ç¬¬ j ä¸ªå­ç©ºé—´çš„é‡åŒ–ä¸­å¿ƒç´¢å¼•
    # codes[i, j] = centroid index of i-th vector in j-th subspace


    for i in range(m):
        # éå†ç¬¬ i ä¸ª PQ å­ç©ºé—´ï¼ˆæ¯ä¸ªåŸå§‹å‘é‡éƒ½ä¼šå‚ä¸è¯¥å¾ªç¯ä¸€æ¬¡ï¼‰
        # Iterate over the i-th PQ subspace (each original vector participates once per subspace)

        # ä»æ‰€æœ‰åŸå§‹å‘é‡ä¸­åˆ‡åˆ†å‡ºç¬¬ i ä¸ªå­ç©ºé—´çš„å­å‘é‡
        # Slice the i-th subspace component from all original vectors
        #
        # è¯­ä¹‰è¯´æ˜ï¼š
        # æ¯ä¸€ä¸ªåŸå§‹å‘é‡ x âˆˆ R^d éƒ½è¢«æŒ‰ç»´åº¦åˆ’åˆ†ä¸º m ä¸ªå­å‘é‡ï¼š
        # Each original vector x âˆˆ R^d is split into m sub-vectors:
        #   x = [x^(0), x^(1), ..., x^(m-1)]
        #
        # å½“å‰å¾ªç¯å¤„ç†ä¸­çš„æ˜¯ç¬¬ i ä¸ªå­å‘é‡ x^(i)
        #
        # ç»´åº¦å˜åŒ–è¯´æ˜ï¼š
        # vectors   çš„å½¢çŠ¶ä¸º (n, d)
        # sub_vecs  çš„å½¢çŠ¶ä¸º (n, d / m)
        sub_vecs = vectors[:, i * subdim:(i + 1) * subdim]

        # å–å‡ºç¬¬ i ä¸ªå­ç©ºé—´å¯¹åº”çš„ PQ å­ç©ºé—´ç æœ¬
        # Retrieve the PQ codebook of the i-th subspace
        #
        # centers çš„å½¢çŠ¶ä¸º (ks, d / m)
        # è¡¨ç¤ºè¯¥å­ç©ºé—´ä¸­é€šè¿‡ KMeans å­¦åˆ°çš„ ks ä¸ªåŸå‹ä¸­å¿ƒ
        centers = codebooks[i]

        # è®¡ç®—ã€Œæ‰€æœ‰å‘é‡çš„ç¬¬ i ä¸ªå­å‘é‡ã€åˆ°ã€Œè¯¥å­ç©ºé—´æ‰€æœ‰ä¸­å¿ƒã€çš„è·ç¦»
        # Compute distances between all i-th sub-vectors and all centroids in this subspace
        #
        # è¿™æ˜¯ PQ ç¼–ç çš„æ ¸å¿ƒæ­¥éª¤ï¼šä¸ºæ¯ä¸ªå­å‘é‡å¯»æ‰¾æœ€è¿‘çš„å­ç©ºé—´ä¸­å¿ƒ
        # This is the core step of PQ encoding: find the nearest centroid for each sub-vector
        #
        # Broadcasting è¿‡ç¨‹è¯¦è§£ï¼š
        # sub_vecs[:, None, :]   â†’ (n, 1, d / m)
        # centers[None, :, :]    â†’ (1, ks, d / m)
        #
        # ä¸¤è€…ç›¸å‡åå¾—åˆ°å·®å€¼å¼ é‡ï¼š
        # (n, ks, d / m)
        #
        # å¯¹æœ€åä¸€ä¸ªç»´åº¦ï¼ˆå­ç©ºé—´ç»´åº¦ï¼‰è®¡ç®— L2 èŒƒæ•°ï¼š
        # â†’ è·ç¦»çŸ©é˜µ shape = (n, ks)
        dists = np.linalg.norm(
            sub_vecs[:, None, :] - centers[None, :, :],
            axis=2
        )

        # å¯¹äºæ¯ä¸€ä¸ªåŸå§‹å‘é‡ï¼Œåœ¨ç¬¬ i ä¸ªå­ç©ºé—´ä¸­é€‰æ‹©è·ç¦»æœ€è¿‘çš„ä¸­å¿ƒç´¢å¼•
        # For each original vector, select the nearest centroid index in the i-th subspace
        #
        # argmin çš„è¯­ä¹‰æ˜¯ï¼š
        # å¯¹æ¯ä¸€è¡Œï¼ˆä¸€ä¸ªå‘é‡çš„å­å‘é‡ï¼‰ï¼š
        #   åœ¨ ks ä¸ªä¸­å¿ƒä¸­é€‰æ‹©è·ç¦»æœ€å°çš„é‚£ä¸ª
        #
        # ç»“æœå«ä¹‰ï¼š
        # codes[j, i] = ç¬¬ j ä¸ªåŸå§‹å‘é‡åœ¨ç¬¬ i ä¸ªå­ç©ºé—´ä¸­çš„ PQ ç¼–ç ç´¢å¼•
        #
        # æ³¨æ„ï¼š
        # è¿™é‡Œåªè®°å½•â€œä¸­å¿ƒç´¢å¼•â€ï¼Œä¸ä¿å­˜è·ç¦»ã€ä¸ä¿å­˜æ®‹å·®
        # Only the centroid index is stored, distances/residuals are NOT stored
        #
        # è‡³æ­¤ï¼Œä¸€ä¸ªåŸå§‹å‘é‡åœ¨è¯¥å­ç©ºé—´ä¸­çš„è¡¨ç¤ºè¢«å‹ç¼©ä¸ºä¸€ä¸ªæ•´æ•°
        # At this point, one subspace of an original vector is compressed into a single integer
        codes[:, i] = np.argmin(dists, axis=1)


    # è‡³æ­¤ï¼ŒPQ ç¼–ç å®Œæˆï¼š
    # At this point, PQ encoding is complete.
    #
    # æ¯ä¸ªå‘é‡è¢«å‹ç¼©ä¸º m ä¸ªæ•´æ•°
    # Each vector is compressed into m integers
    #
    # å­˜å‚¨å¤æ‚åº¦ï¼š
    # Storage complexity:
    #   åŸå§‹å‘é‡ï¼šn Ã— d Ã— 4 bytes
    #   PQ ç¼–ç ï¼š n Ã— m Ã— 4 bytes
    #
    # åœ¨ ANN ç³»ç»Ÿä¸­ï¼Œè¿™æ˜¯æ€§èƒ½ä¸å†…å­˜æƒè¡¡çš„æ ¸å¿ƒ
    # This is the core memoryâ€“accuracy tradeoff in ANN systems

    return codes



def build_ivf_pq(vectors, nlist, m, ks):
    # æ„å»º IVF-PQ ç´¢å¼•çš„å®Œæ•´æµç¨‹
    # Build the full IVF-PQ index pipeline
    #
    # IVF-PQ çš„æ•´ä½“ç›®æ ‡ï¼š
    # Overall goal of IVF-PQ:
    # å°†å¤§è§„æ¨¡é«˜ç»´å‘é‡é›†åˆæ‹†è§£ä¸ºï¼š
    # Decompose a large high-dimensional vector set into:
    #
    # â‘  IVFï¼ˆInverted Fileï¼‰ï¼šå‡å°‘æœç´¢ç©ºé—´
    # â‘¡ PQï¼ˆProduct Quantizationï¼‰ï¼šå‹ç¼©å‘é‡ä»¥æ”¯æŒå¿«é€Ÿæ’åº
    #
    # æœ¬å‡½æ•°å®Œæˆä¸‰ä»¶ä¸å¯ç¼ºå¤±çš„äº‹æƒ…ï¼š
    # This function performs three indispensable steps:
    #
    # 1) è®­ç»ƒ IVF çš„ coarse quantizerï¼ˆç¬¬ä¸€æ¬¡ KMeansï¼‰
    # 2) è®­ç»ƒ PQ çš„ subspace codebooksï¼ˆm æ¬¡ KMeansï¼‰
    # 3) å°†æ‰€æœ‰å‘é‡ç¼–ç å¹¶å†™å…¥ inverted lists

    print(f"ğŸ“¦ [build_ivf_pq] nlist={nlist}, m={m}, ks={ks}, samples={len(vectors)}")
    # æ‰“å°ç´¢å¼•è§„æ¨¡å…³é”®å‚æ•° / Print index scale parameters

    # ================================
    # Step 1ï¼šè®­ç»ƒ IVF ç²—é‡åŒ–å™¨
    # Step 1: Train IVF coarse quantizer
    # ================================

    # ç›®æ ‡ï¼š
    # Assign each vector to exactly one coarse centroid
    # ç”¨ä¸€ä¸ªè¾ƒå°çš„ nlist å°†ç©ºé—´ç²—åˆ†æˆå¤šä¸ª Voronoi åŒºåŸŸ
    #
    # æ•°å­¦å½¢å¼ï¼š
    # cid(x) = argmin_j || x - C_j ||
    #
    # å¾—åˆ°çš„ cid ä»…ç”¨äº routingï¼Œè€Œéç²¾ç¡®è¡¨ç¤º
    coarse_centers = kmeans(vectors, nlist)

    # coarse_centers å½¢çŠ¶ï¼š
    # shape = (nlist, d)
    #
    # è¯­ä¹‰ï¼š
    # æ¯ä¸€ä¸ª coarse_center æ˜¯ä¸€ä¸ªå…¨ç»´åº¦çš„ä»£è¡¨ç‚¹
    # Each coarse_center represents a large region in the vector space

    # ================================
    # Step 2ï¼šè®­ç»ƒ PQ å­ç©ºé—´ç æœ¬
    # Step 2: Train PQ subspace codebooks
    # ================================

    # PQ çš„ç›®æ ‡ï¼š
    # Approximate each vector with a sum of low-dimensional codewords
    #
    # å°† d ç»´å‘é‡åˆ‡æˆ m ä¸ªå­ç©ºé—´ï¼š
    # Split d-dim vector into m subspaces
    #
    # æ¯ä¸ªå­ç©ºé—´ä¸Šç‹¬ç«‹è®­ç»ƒä¸€ä¸ª KMeans
    pq_codebooks = train_pq(vectors, m, ks)

    # pq_codebooks çš„ç»“æ„ï¼š
    # pq_codebooks[m][ks][subdim]
    #
    # å³ï¼š
    # m ä¸ªå­ç©ºé—´
    # æ¯ä¸ªå­ç©ºé—´ ks ä¸ªä¸­å¿ƒ
    # æ¯ä¸ªä¸­å¿ƒç»´åº¦ä¸º d / m

    # ================================
    # Step 3ï¼šPQ æ‰¹é‡ç¼–ç æ‰€æœ‰å‘é‡
    # Step 3: Batch PQ-encode all vectors
    # ================================

    print("ğŸ§± [build_ivf_pq] start batch PQ encoding")

    # å¯¹æ¯ä¸ªå‘é‡è®¡ç®— m ä¸ªå­ç©ºé—´ç´¢å¼•
    # Encode each vector into m subspace centroid indices
    #
    # è¾“å‡ºï¼š
    # pq_codes shape = (N, m)
    pq_codes = pq_encode_batch(vectors, pq_codebooks)

    # ================================
    # Step 4ï¼šè®¡ç®—æ‰€æœ‰å‘é‡çš„ coarse assignment
    # Step 4: Compute coarse assignments for all vectors
    # ================================

    # ä½¿ç”¨å‘é‡åŒ–æ–¹å¼è®¡ç®—æ‰€æœ‰å‘é‡ â†’ æ‰€æœ‰ coarse centers çš„è·ç¦»
    # Compute distance matrix in a fully vectorized way
    #
    # (N, 1, d) - (1, nlist, d) â†’ (N, nlist)
    dists = np.linalg.norm(
        vectors[:, None, :] - coarse_centers[None, :, :],
        axis=2
    )

    # å¯¹æ¯ä¸ªå‘é‡é€‰æ‹©æœ€è¿‘çš„ coarse ä¸­å¿ƒ
    # For each vector, choose nearest coarse centroid
    #
    # assignments shape = (N,)
    assignments = np.argmin(dists, axis=1)

    # ================================
    # Step 5ï¼šæ„å»ºå€’æ’è¡¨ï¼ˆInverted Listsï¼‰
    # Step 5: Build inverted lists
    # ================================

    # åˆå§‹åŒ– nlist ä¸ªå€’æ’é“¾è¡¨
    # Initialize nlist inverted lists
    inverted_lists = [[] for _ in range(nlist)]

    # å°†æ¯ä¸ªå‘é‡å†™å…¥å¯¹åº”çš„å€’æ’åˆ—è¡¨
    # Insert each vector into its assigned inverted list
    #
    # å€’æ’è¡¨ä¸­å­˜å‚¨çš„ä¿¡æ¯ï¼š
    # (åŸå§‹å‘é‡ ID, PQ ç¼–ç )
    #
    # è€Œä¸æ˜¯å­˜å®Œæ•´å‘é‡
    for idx, list_id in enumerate(assignments):
        inverted_lists[list_id].append((idx, pq_codes[idx]))

    # ================================
    # IVF-PQ ç´¢å¼•æ„å»ºå®Œæˆ
    # IVF-PQ index construction finished
    # ================================

    print("âœ… [build_ivf_pq] index built successfully")

    # è¿”å›ç´¢å¼•ä¸‰è¦ç´ ï¼š
    # Return three essential index components:
    #
    # 1) coarse_centers    â†’ æœç´¢é˜¶æ®µç”¨äº list routing
    # 2) pq_codebooks      â†’ æœç´¢é˜¶æ®µç”¨äº ADC è·ç¦»è®¡ç®—
    # 3) inverted_lists    â†’ å€™é€‰é›†æ¥æº
    return coarse_centers, pq_codebooks, inverted_lists



def pq_adc_distance(query, code, codebooks):
    # ä½¿ç”¨ ADCï¼ˆAsymmetric Distance Computationï¼‰è®¡ç®—æŸ¥è¯¢å‘é‡ä¸ PQ ç¼–ç å‘é‡ä¹‹é—´çš„è¿‘ä¼¼è·ç¦»
    # Compute approximate distance between a query vector and a PQ-encoded vector using ADC
    #
    # ADC çš„æ ¸å¿ƒæ€æƒ³ï¼š
    # Core idea of ADC:
    #   - æŸ¥è¯¢å‘é‡ query ä¿æŒä¸ºã€ŒåŸå§‹é«˜ç²¾åº¦æµ®ç‚¹å‘é‡ã€
    #   - æ•°æ®åº“å‘é‡ x ä½¿ç”¨ PQ è¿›è¡Œå‹ç¼©ï¼Œä»…ä¿ç•™ m ä¸ªä¸­å¿ƒç´¢å¼•
    #   - è·ç¦»è®¡ç®—æ—¶ï¼Œä¸é‡å»ºå®Œæ•´å‘é‡ï¼Œè€Œæ˜¯é€å­ç©ºé—´ç´¯åŠ è·ç¦»
    #
    # ç›®æ ‡è¿‘ä¼¼å…¬å¼ï¼š
    # Approximation formula:
    #   ||q - x||Â² â‰ˆ Î£_{i=0}^{m-1} || q^(i) - c^(i)_{code[i]} ||Â²
    #
    # å…¶ä¸­ï¼š
    #   q^(i)         : æŸ¥è¯¢å‘é‡åœ¨ç¬¬ i ä¸ªå­ç©ºé—´çš„å­å‘é‡
    #   c^(i)_{code[i]} : PQ codebook ä¸­ç¬¬ i ä¸ªå­ç©ºé—´è¢«é€‰ä¸­çš„ä¸­å¿ƒ
    #
    # æ³¨æ„ï¼š
    #   æœ¬å®ç°ä½¿ç”¨çš„æ˜¯ L2 è·ç¦»ï¼ˆéå¹³æ–¹ï¼‰ï¼Œä»¥ä¾¿ä¸å‰ç»­ä»£ç ä¿æŒä¸€è‡´
    #   This implementation uses L2 distance (not squared) for consistency

    d = query.shape[0]
    # d ä¸ºåŸå§‹å‘é‡ç»´åº¦
    # d is the dimensionality of the original vectors

    m = len(codebooks)
    # m ä¸º PQ å­ç©ºé—´æ•°é‡
    # m is the number of PQ subspaces

    subdim = d // m
    # subdim ä¸ºæ¯ä¸ªå­ç©ºé—´çš„ç»´åº¦
    # subdim is the dimensionality of each subspace

    dist = 0.0
    # åˆå§‹åŒ–ç´¯ç§¯è·ç¦»
    # Initialize accumulated distance

    for i in range(m):
        # éå†æ¯ä¸€ä¸ª PQ å­ç©ºé—´
        # Iterate over each PQ subspace

        # ä»æŸ¥è¯¢å‘é‡ä¸­å–å‡ºç¬¬ i ä¸ªå­ç©ºé—´çš„å­å‘é‡
        # Extract the i-th sub-vector of the query
        #
        # query çš„ç»“æ„ï¼š
        # query = [q^(0), q^(1), ..., q^(m-1)]
        q_sub = query[i * subdim:(i + 1) * subdim]

        # æ ¹æ® PQ ç¼–ç ï¼Œä»ç¬¬ i ä¸ªå­ç©ºé—´çš„ codebook ä¸­å–å‡ºå¯¹åº”ä¸­å¿ƒ
        # Retrieve the corresponding centroid from the i-th subspace codebook using PQ code
        #
        # code[i] æ˜¯ä¸€ä¸ªæ•´æ•°ç´¢å¼•ï¼Œè¡¨ç¤ºï¼š
        #   åŸå§‹æ•°æ®åº“å‘é‡åœ¨ç¬¬ i ä¸ªå­ç©ºé—´ä¸­è¢«é‡åŒ–åˆ°å“ªä¸ªä¸­å¿ƒ
        center = codebooks[i][code[i]]

        # è®¡ç®—æŸ¥è¯¢å­å‘é‡ä¸è¯¥å­ç©ºé—´ä¸­å¿ƒä¹‹é—´çš„ L2 è·ç¦»
        # Compute L2 distance between the query sub-vector and the selected centroid
        #
        # è¿™ä¸€æ­¥ä¸ä¼šè®¿é—®æ•°æ®åº“åŸå§‹å‘é‡
        # This step does NOT access the original database vectors
        #
        # å­ç©ºé—´è·ç¦»æ˜¯ç‹¬ç«‹çš„ã€å¯åŠ çš„
        # Subspace distances are independent and additive
        dist += np.linalg.norm(q_sub - center)

    # dist å³ä¸ºè¿‘ä¼¼çš„æŸ¥è¯¢-æ•°æ®åº“å‘é‡è·ç¦»
    # dist is the approximate distance between query and database vector

    return dist
    # è¿”å› ADC è®¡ç®—å¾—åˆ°çš„è¿‘ä¼¼è·ç¦»
    # Return the approximate distance computed by ADC


def search_ivf_pq(query, coarse_centers, codebooks, inverted_lists, topk=5, nprobe=4):
    # IVF-PQ æŸ¥è¯¢æµç¨‹
    # IVF-PQ search pipeline
    print(f"ğŸ” [search_ivf_pq] topk={topk}, nprobe={nprobe}")

    coarse_dists = np.linalg.norm(coarse_centers - query, axis=1)
    probe_ids = np.argsort(coarse_dists)[:nprobe]

    results = []
    for pid in probe_ids:
        for idx, code in inverted_lists[pid]:
            dist = pq_adc_distance(query, code, codebooks)
            results.append((idx, dist))

    results.sort(key=lambda x: x[1])
    print("âœ… [search_ivf_pq] finished")
    return results[:topk]


if __name__ == "__main__":
    # ä¸»å…¥å£ / Main entry
    np.random.seed(42)

    path = "test.txt"
    if not os.path.exists(path):
        raise FileNotFoundError("test.txt not found")

    # âœ… å·²éªŒè¯ 700 ä¸‡ä¸­æ–‡å­—ç¬¦ä¸ä¼šå¡æ­»
    chunks = chunk_utf8_file(path, 512)

    dim = 128
    vectors = np.array([text_to_vector(t, dim) for t in chunks])

    coarse, pq_books, ivf = build_ivf_pq(
        vectors=vectors,
        nlist=64,
        m=8,
        ks=16
    )

    query = vectors[0]
    result = search_ivf_pq(
        query,
        coarse,
        pq_books,
        ivf,
        topk=5,
        nprobe=4
    )

    print("ğŸ¯ Search Result:", result)
